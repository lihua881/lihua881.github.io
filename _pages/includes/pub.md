
<span class='anchor' id='-lw'></span>
# üìù ÂèëË°®ËÆ∫Êñá 


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='/paperimgs/tmanet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TMANet: Triple Multi-Scale Attention based Network with Boundary Association Loss for Superpixel Segmentation](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10888234)

Ziyi Zhang, Shijie Lian, **Hua Li***

[**Code**]([‰ª£Á†ÅÈìæÊé•])
  - This paper proposes a Triple Multi-Scale Attention-based Network (TMANet) for superpixel segmentation, which incorporates Triple Multi-Scale Attention (TMA) and Boundary Association (BA) loss to effectively capture fine-grained contextual information and enhance boundary accuracy.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICME 2025</div><img src='paperimgs/xxx.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OptiDiff: Unsupervised Deep-Sea Image Enhancement via Optical Priors Guided Stable Diffusion]([PDFÈìæÊé•])

Wenhui Wu, Yuemiao Wang, **Hua Li***, Yuanhao Gong

[**Code**]([‰ª£Á†ÅÈìæÊé•])

- OptiDiff is a Unsupervised Deep-Sea Image Enhancement method via Optical Priors Guided Stable Diffusion.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='/paperimgs/diving.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Diving into Underwater: Segment Anything Model Guided Underwater Salient Instance Segmentation and A Large-scale Dataset](https://arxiv.org/pdf/2406.06039)

Shijie Lian, Ziyi Zhang, **Hua Li***, Wenjie Li, Laurence Tianruo Yang, Sam Kwong, Runmin Cong.

[**Code**](https://github.com/LiamLian0727/USIS10K)
  - This paper introduces the first large-scale underwater salient instance segmentation dataset (USIS10K) and proposes the USIS-SAM architecture based on the Segment Anything Model (SAM) for underwater vision tasks. The model incorporates an Underwater Adaptive Visual Transformer (UA-ViT) and an automatic Salient Feature Prompter Generator (SFPG) to improve segmentation accuracy in complex underwater environments.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Frontiers in Marine Science</div><img src='/paperimgs/usnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[USNET: Underwater Image Superpixel Segmentation via Multi-scale Water-net](https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2024.1411717/full)

Chuhong Wang, Wenli Duan, Chengche Luan, Junyan Liang, Lengyu Shen, **Hua Li***

[**Code**](xxx)
  - This paper proposes the first underwater superpixel segmentation network (USNet) designed to address the unique challenges of underwater image quality degradation. The network incorporates a multi-scale water-net module (MWM) for image enhancement, a degradation-aware attention (DA) mechanism to handle light scattering and absorption, and dynamic spatiality embedding to combine deep and shallow spatial features.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='/paperimgs/esnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ESNet: Evolution and Succession Network for High-Resolution Salient Object Detection](https://dl.acm.org/doi/10.5555/3692070.3693315)

Hongyu Liu, Runmin Cong*, **Hua Li**, Qianqian Xu, Qingming Huang, Wei Zhang

[**Code**](https://github.com/big-feather/ESNet_ICML24)
  - This paper presents a two-stage High-Resolution Salient Object Detection (HRSOD) model, consisting of an evolution stage with a Low-resolution Location Model (LrLM) for detail-preserving localization, and a succession stage with a High-resolution Refinement Model (HrRM) to enhance and refine features for final saliency prediction. A new evaluation metric, Boundary-Detail-aware Mean Absolute Error (MAEBD), is introduced to assess the model's ability to preserve details.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE Transactions on Multimedia</div><img src='/paperimgs/stereo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Stereo Superpixel Segmentation Via Decoupled Dynamic Spatial-Embedding Fusion Network](https://ieeexplore.ieee.org/document/10098144)

**Hua Li**, Junyan Liang, Ruiqi Wu, Runmin Cong*, Wenhui Wu; Sam Tak Wu Kwong

[**Code**](xxx)
  - This paper introduces a stereo superpixel segmentation method that incorporates a decoupling mechanism to separately handle stereo disparity and spatial information. A Decoupled Stereo Fusion Module (DSFM) is proposed to align stereo features and address occlusion issues, while a Dynamic Spatiality Embedding Module (DSEM) is designed to reintroduce spatial information with adaptive weight adjustment for finer segmentation. This approach improves the segmentation performance by efficiently utilizing both stereo disparity and spatial features.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM Multimedia 2023</div><img src='/paperimgs/fsnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FSNet: Frequency Domain Guided Superpixel Segmentation Network for Complex Scenes](https://dl.acm.org/doi/10.1145/3581783.3613826)

**Hua Li**, Junyan Liang, Wenjie Li, Wenhui Wu*

[**Code**](xxx)
  - This paper proposes FSNet, an end-to-end frequency domain guided superpixel segmentation network designed for complex scenes. FSNet generates superpixels with sharp boundary adherence by fusing deep features from both spatial and frequency domains. An improved frequency information extractor (IFIE) captures frequency domain details with sharp boundary features, while a dense hybrid atrous convolution (DHAC) block preserves semantic information by capturing broader and deeper spatial features. The fusion of these features enables the generation of semantic perceptual superpixels with enhanced boundary accuracy.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='/paperimgs/watermask.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[WaterMask: Instance Segmentation for Underwater Imagery](https://openaccess.thecvf.com/content/ICCV2023/papers/Lian_WaterMask_Instance_Segmentation_for_Underwater_Imagery_ICCV_2023_paper.pdf)

Shijie Lian, **Hua Li***, Runmin Cong*, Suqi Li, Wei Zhang, Sam Kwong

[**Code**](https://github.com/LiamLian0727/WaterMask)
  - This paper introduces the first underwater image instance segmentation dataset (UIIS), consisting of 4,628 images across 7 categories with pixel-level annotations. The authors also propose WaterMask, a novel method for underwater image instance segmentation. WaterMask utilizes the Difference Similarity Graph Attention Module (DSGAT) to recover lost details from image degradation and downsampling. Additionally, a Multi-level Feature Refinement Module (MFRM) is designed to predict foreground and boundary masks at different scales, guided by a Boundary Mask Strategy (BMS) with boundary learning loss to enhance segmentation accuracy. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE Signal Processing Letters</div><img src='/paperimgs/sceir.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Atmospheric Scattering Model Induced Statistical Characteristics Estimation for Underwater Image Restoration](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10138650)

Shuaibo Gao, Wenhui Wu, **Hua Li**, Linwei Zhu, Xu Wang

[**Code**](https://github.com/charliewalker322/SCEIR-pytorch)
  - This paper proposes a novel lightweight model for Underwater Image Restoration (UIR) by establishing a statistical relationship between underwater and recovered images based on the Atmospheric Scattering Model (ASM), avoiding the need to estimate transmission maps and global light. The UIR problem is addressed through two modules: global restoration and local compensation.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICME 2021</div><img src='/paperimgs/dualattn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<span style="color:red">(Oral)</span>[Stereo Superpixel Segmentation via Dual-Attention Fusion Networks](https://ieeexplore.ieee.org/document/9428302)

Ruiqi Wu, Yajuan Du, **Hua Li***, Yucong Dai

[**Code**](xxx)
  - This paper proposes an end-to-end dual-attention fusion network for stereo image superpixel segmentation, aiming to generate parallax-consistency superpixels by leveraging depth information from stereo image pairs. The method extracts deep features from both left and right views using a convolutional network, and integrates these features using a parallax attention and channel attention mechanism. The stereo superpixels are then generated through a differentiable clustering algorithm, which is fully trainable with deep learning techniques.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Information Sciences</div><img src='/paperimgs/superpixel.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Stereo Superpixel: An Iterative Framework based on Parallax Consistency and Collaborative Optimization](https://www.sciencedirect.com/science/article/pii/S002002552031197X)

**Hua Li**, Runmin Cong*, Sam Kwong*, Chuanbo Chen, Qianqian Xu, Chongyi Li

[**Code**](xxx)
  - This paper introduces a left-right interactive optimization framework for stereo superpixel segmentation, focusing on parallax consistency between the left and right views. The framework divides the images into paired and non-paired regions, and employs a collaborative optimization scheme to refine the matched superpixels interactively. This approach, which is the first to consider parallax consistency in stereo superpixel segmentation, demonstrates superior performance in both consistency and accuracy compared to traditional single-image segmentation methods through quantitative and qualitative experiments.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE Transactions on Industrial Informatics</div><img src='/paperimgs/subspace.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Superpixel Segmentation Based on Spatially Constrained Subspace Clustering](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9292086)

**Hua Li**, Yuheng Jia, Runmin Cong, Wenhui Wu, Sam Kwong*, Chuanbo Chen

[**Code**](xxx)
  - This article proposes a novel approach to superpixel segmentation by treating each representative region with independent semantic information as a subspace, formulating the segmentation as a subspace clustering problem to better preserve detailed content boundaries. The authors address the issue of boundary confusion and segmentation errors caused by ignoring the spatial correlation of pixels within a superpixel. To overcome this, they introduce a spatial regularization and propose a convex locality-constrained subspace clustering model that ensures adjacent pixels with similar attributes are clustered together, resulting in content-aware superpixels with more detailed boundaries. The model is efficiently solved using an alternating direction method of multipliers solver.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing</div><img src='/paperimgs/fusionnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A parallel down-up fusion network for salient object detection in optical remote sensing images](https://www.sciencedirect.com/science/article/abs/pii/S0925231220313692)

Chongyi Li, Runmin Cong*, Chunle Guo, **Hua Li***, Chunjie Zhang, Feng Zheng, Yao Zhao

[**Code**](xxx)
  - This paper proposes a novel Parallel Down-up Fusion network (PDF-Net) for salient object detection (SOD) in optical remote sensing images (RSIs), addressing challenges such as diverse spatial resolutions, object scales, and cluttered backgrounds. PDF-Net utilizes five parallel paths with successive down-sampling to capture salient objects at different scales. Dense connections are employed to leverage both low- and high-level features, while cross-path relations enhance feature representation. The network fuses multi-resolution features, combining the strengths of high-resolution details and low-resolution context to improve detection accuracy. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE Transactions on Multimedia</div><img src='/paperimgs/asymmetric.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Superpixel Segmentation Based on Square-Wise Asymmetric Partition and Structural Approximation](https://ieeexplore.ieee.org/document/8673630)

**Hua Li**, Sam Kwong*, Chuanbo Chen, Yuheng Jia*, Runmin Cong

[**Code**](xxx)
  - This paper presents a superpixel segmentation method aimed at generating structural superpixels with sharp boundary adherence and comprehensive semantic information. The segmentation is formulated as a square-wise asymmetric partition problem, where semantic perceptual superpixels are organized in square units to preserve rich semantic content while minimizing storage. To ensure regular-shaped superpixels that align well with image boundaries and contours, a combinatorial optimization strategy is proposed to optimally combine squares and isolated pixels. This approach addresses the irregularity issue in traditional superpixel methods, enhancing boundary precision and semantic coherence.
</div>
</div>
